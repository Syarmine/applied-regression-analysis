---
title: "01_linear_regression"
output:
  html_document: default
  pdf_document: default
date: "2024-09-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

The dataset used this week is HIEDATA SHORT which is an excerpt from the RAND Health Insurance
Experiment — a large, federally funded study conducted in the United States between 1971 and 1982. It
contains data that are used to illustrate linear regression. For more details on using R Markdown see <https://www.rand.org/health-care/projects/hie.html>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
# setwd("C:~/applied-regression-analysis")
load(file.path("data",'hiedata_short.Rdata'))
```

## Introdution to the task

To fit linear regression models for which the quantitative response variable is family income (i.e., income
in the data frame hiedata_short). The variables education, age, and ghi (i.e., General Health Index) will
be used as explanatory variables.

  - `income`: A respondent’s family income recorded in thousands of 1973 dollars (i.e., income×1000 will
give you the number of 1973 dollars).
  - `education`: A respondent’s education in years of school completed.
  - `age` : A respondent’s age in years.
  - `ghi`: A respondent’s self-assessed general health based on a set of 22 survey questions. Each question
was answered on a five-point scale, and the results were then summed across the 22 items. Higher
scores represent better self-reported health


```{r pressure, echo=TRUE}
# Look at the variable names in the data frame
# Variables/covariates are the columns
# Cases/respondents are the rows
colnames(hiedata_short)
```

## Manipulating Data Frames

To access variables in the data frame, you can type the data frame variable, followed by the $ operator, and
then the name of the variable. Note, you can also access variables in a data frame by indexing specific rows and columns by their name
with the [,] operator — where row names are placed before the comma and column names are placed after the comma.

```{r, echo=TRUE}
summary(hiedata_short$income)
```

```{r}
# We can inspect multiple variables using comma-separated names within the `c()`command.
summary(hiedata_short[, c("income", "age")])
```

## Fitting Regression Models Using R
To estimate a simple linear regression model, we use the lm() function. In this function, we first supply the
“formula” (or regression equation) in the form y ~ x1 + x2 + x3 + ... (where y and x1/x2/x3 are the
response and explanatory variable names respectively). Then we supply the data frame where these variables
can be found. The results of this function can be “stored” in a new object (in the example below, we call this
lm.mod) so that we can access them later, e.g.:
```{r}
lm.mod <- lm(formula = income ~ age, # Define the regression equation/formula
data = hiedata_short # Feed in data frame where the variable names exist
)
```

## Analysing regression models in R
We can inspect the information contained within a regression model in multiple ways. Most often, you will
want to use the function summary(). However, you can also simply type the name of the stored regression
model to see a basic breakdown of the results.

```{r}
lm.mod # Basic output
summary(lm.mod) # Detailed output
```
To calculate confidence intervals, you can use the function confint():
```{r}
confint(object = lm.mod, # The model we want the confidence interval for
parm = c("x"), # The set of variables we want confidence intervals for
level = 0.95 # The desired level of confidence
)
```

## At-Home Exercise
### At-Home Exercise 1
Calculate basic summary statistics for the variables income, education, age, and ghi using the function
summary().
- A simple descriptive statistics will help us learn more about the data — and reviewing
descriptive statistics is always useful as a preliminary step when carrying out complex
analyses!

```{r}
summary(hiedata_short[,c("income", "education", "age", "ghi")])
```
Answer: The output above gives basic information about key variables in `hiedata_short`. Note, in particular, the range of the variables ghi and age. 

### At-Home Exercise 2
To create a scatterplot in base R, we use the plot() command. 

Create a scatter plot of income (on the y-axis) against education (on the x-axis) using the function plot().
- Do the two variables appear to have a linear relationship?
- Would you anticipate, after looking at the scatter plot, that the Pearson correlation coefficient
between income and education is positive, near zero, or negative?
```{r}
plot(x = hiedata_short$education,
y = hiedata_short$income,
type = "p", # "p" = Points
xlab = "Years of Education",
ylab = "Family Income (1000s of USD)",
main = "Income Versus Education\nRAND Health Insurance Experiment\n",
)
```

- What does the fitted regression line tell us?

```{r}
plot(x = hiedata_short$education,
y = hiedata_short$income,
type = "p", # What type of plot? "p" = Points; Run: `?plot` for more details
col = "black", # Colour of the points
xlab = "Years of Education", # Add title for the x-axis
ylab = "Family Income (1000s of USD)", # Add title for the y-axis
main = "Income Versus Education\nRAND Health Insurance Experiment\n", # Add a plot title. "\n" = Insert line break
)

# Fit the basic regression
lm.educ <- lm(formula = income ~ education,
data = hiedata_short
)
# Add least squares fitted line to plot
abline(reg = lm.educ, # The saved regression model
lwd = 2, # Width of the regression line on the plot
col = "purple" # The color of the regression line on the plot
)

```

Answers: It is very difficult to tell whether the variables have a linear relationship
from the scatter plot alone — especially as there is a lot of variation in family income amongst
people with the same level of education. Still, the regression line indicates a positive association.

Accordingly, in this sample, people with more years of education are predicted to have, on average,
higher levels of family income, thus the upward-sloping regression line. The slope coefficient for
the regression line and the Pearson correlation coefficient have the same sign as the slope is from
a simple linear regression model (see Page 39 of the MY452 Coursepack). Thus, the Pearson
correlation coefficient for income and education is also positive

## At-Home Exercise 3
First, fit a simple linear regression model of income against education.
Then, identify the estimates for the intercept/constant term (i.e., α or β0) and the regression coefficient for education (i.e., βeducation).
- How do these estimated quantities relate to the fitted regression line in Exercise 2?
Finally, identify and discuss the following quantities from the model output:
• The 95% confidence interval for the estimate of βeducation
• The t-test statistic for the estimate of βeducation and its associated p-value
• The coefficient of determination (R2)

```{r}
# SOLUTIONS:
# Run a simple linear regression model
lm.educ <- lm(formula = income ~ education,
data = hiedata_short
)
summary(lm.educ) # Recall the detailed output
```

ANSWER:
• Estimated Constant Term: αˆ = 4.367
• Estimated Regression Coefficient for Education: βˆ = 0.481
The intercept (i.e., α, β0) indicates where the regression line of best fit in Exercise 2 intersects
the y-axis. The coefficient for education (βeducation) is the slope of this line and it indicates the
rate of change in income relative to education.

The interpretation of βˆ for education (0.481) is that, for one additional year of full-time education
the expected family income will be higher by $481 (i.e. 0.481 × $1000).

The 95% confidence interval for the coefficient for education is (0.375; 0.588) or, rather, $375 to
$588 — i.e., (0.375 × $1000; 0.588 × $1000). We are therefore 95% confident that the true model
parameter lies between $375 and $588.

[Note “We are 95% confident. . . ” means that, if we followed this procedure to construct the
confidence interval over many repeated samples, 95% of these confidence intervals would contain
the true population parameter.]

The t-statistic for the education regression coefficient is 8.863. The corresponding p-value is
less than 0.001. The p-value of the t-test is very small, providing strong evidence against the null hypothesis that the population value of the coefficient for education is equal to 0 (i.e.,
H0 : βˆEducation = 0). Indeed, it is smaller than any conventionally used significance level in the
social sciences (i.e., 0.05, 0.01, and 0.001). Therefore, we can reject the null hypothesis.

Accordingly, there is very strong statistical evidence that one’s level of education and family
income are correlated in the population. This is also indicated by the fact that the confidence
interval does not contain the value of 0.

Finally, an R2 of 0.044 indicates that 4.4% of the variation in the values of family income in the
sample is explained by variation in the respondents’ levels of education. This proportion is not
very large (as we can see in the scatterplot in Exercise 2).

Thus, level of education does not appear to be very useful for predicting an individual’s family
income for these particular respondents — even though there is strong statistical evidence of an
association between the two variables.

## At-Home exercise 4
Fit a multiple linear regression model for income given education, age, and ghi.
• Identify the same elements of the output as you did in Exercise 3. However, note that your new model
now includes coefficients for all three explanatory variables.
```{r} 
# The same as if we were writing the regression equation, note we add variables using `+`
lm.all <- lm(formula = income ~ education + age + ghi,
data = hiedata_short
)
summary(lm.all)
```

```{r}
confint(object = lm.all,
parm = c("education", "age", "ghi"),
level = 0.95
)```

ANSWER:
• Estimated Constant Term: αˆ = -4.606
• Estimated Regression Coefficient for Education: βˆ = 0.560
• Test Statistics for Coefficient for Education: t = 10.38
• p-value for Coefficient for Education: p < 0.001
• 95% Confidence Interval for Coefficient for Education: (0.454; 0.665)
• Estimated Regression Coefficient for Age: βˆ = 0.146
• Test Statistics for Coefficient for Age: t = 12.46
• p-value for Coefficient for Age: p < 0.001
• 95% Confidence Interval for Coefficient for Age: (0.123; 0.169)
• Estimated Regression Coefficient for General Health Index: βˆ = 0.040
• Test Statistics for Coefficient for General Health Index: t = 4.35
• p-value for Coefficient for General Health Index: p < 0.001
• 95% Confidence Interval for Coefficient for General Health Index: (0.022; 0.058)
• Coefficient of Determination: R2 = 0.130
```

## At-Home Exercise 5
The code below visualize the fitted/predicted values (i.e., yˆi) for income as education varies between
0 and 25 years while age and ghi are respectively fixed at the values of 30 and 70.

First, use the function seq() to generates a set of ages that range from A to B, in steps of size C
```{r}
# Create a range of values of education from 0 to 25 in increments of 1 year
education.pts <- seq(from = 0, # A (starting value)
to = 25, # B (ending value)
by = 1 # C (value determining the size of the sequence steps)
)
# View education.pts — i.e., a vector of 26 values going from 0 (years of education) to 25
print(education.pts)
```

Second, create a new data frame that contains the range of values of education for which we want the
fitted values, as well as any other explanatory variables that is being used in the model:
```{r}
# Note: R will automatically repeat values to match the length of other columns
# So we can just set a single age and ghi value.
fitted.values.data <- data.frame(age = 30, # Create predictions holding constant `age`
education = education.pts,
ghi = 70
)

```

```{r}
# Check that the first 5 lines of this data frame contain what you expect
head(fitted.values.data, 5)

```

Third, use the function predict() to calculate the fitted/predicted values for income for this data frame of
observations:
```{r}
fitted.values <- predict(object = lm.all, # The saved multiple regression model object
newdata = fitted.values.data, # Desired values of the covariates
interval = "confidence", # https://stats.stackexchange.com/a/16498
level = 0.95 # Desired confidence level
)

fitted.values <- data.frame(fitted.values) # Feed the matrix to `data.frame()`
```
Note that predict() returns the 95% confidence interval around the predicted mean of income
given education, age, and ghi. To construct an interval for a single value, we would need to
change the argument interval = "confidence" to interval = "prediction". For additional
details, see this explainer on the very-useful blog STHDA

```{r}
plot(x = education.pts, # Make the x-axis the covariate that we vary
y = fitted.values$fit, # Extract the fitted values using the `$` operator
type = "l", # What type of plot? "l" = lines; Run: `?plot` for more details
col = "black", # Colour of the line
xlab = "Years of Education", # Add title for the x-axis
ylab = "Predicted Family Income (1000s of USD)", # Add title for the y-axis

# Add a plot title. "\n" = Insert line break
main = "Predicted Income Given Education\nRAND Health Insurance Experiment \n",
sub = "Age fixed at 30; GHI fixed at 70", # Let the reader know what is fixed
ylim = c(0, 20) # The upper and lower limits of the y-axis
)
matlines(education.pts,
fitted.values[, c("lwr", "upr")],
lty = "dashed",
col = "red",
lwd = 2)
rug(x = jitter(hiedata_short$education, factor = 1, amount = NULL),
ticksize = 0.03,
side = 1,
lwd = 0.5)
legend("topright",
legend = c("95% CI"),
col = c("red"),
10
lty = 2,
lwd = 2,
bty = "n" # Draw a box? "n" = "no"
)
```

